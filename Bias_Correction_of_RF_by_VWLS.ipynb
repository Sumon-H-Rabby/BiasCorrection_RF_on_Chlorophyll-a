{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Disclaimer:\n",
        "1) This code only presents the new bias correction methods introduced in the manuscript: **\"Variance-Weighted Least Square: A new point-scale bias correction method to improve surface water quality predictions using ensemble machine learning models\"**.\n",
        "\n",
        "2) The associated other steps, such as splitting data into test and train,four ensemble learning model development, hyperparameter tuning using BayesSearchCV(), performance evaluation and residual analysis, application of other exisiting bias correction methods, comprehensive assessment of bias, and visualizations are not demonstarted here as they followed the standard procedures."
      ],
      "metadata": {
        "id": "Um_aEcaWaLH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Necessary Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import KFold\n"
      ],
      "metadata": {
        "id": "yoGfkas9aGNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the features, target and the pre-selected hyperparameter.\n",
        "CFG = {\n",
        "    \"features\": [\n",
        "        'DO(mg/l)', 'Temperature (deg cels)', 'Salinity(ppt)', 'pH',\n",
        "        'Turbidity(NTU)', 'Phosphate (mg/L)', 'Ammonium (mg/L)',\n",
        "        'Nitrate+Nitrite (mg/L)', 'TotPAR_max', 'ATemp_max',\n",
        "        'RH_mean', 'BP_mean', 'Discharge (cfs)'\n",
        "    ],\n",
        "    \"target\": \"Chlorophyll-a (ug/L)\",\n",
        "    \"rf_base\": dict(n_estimators=256, max_depth=20, min_samples_leaf=1, min_samples_split=2 , random_state=42, n_jobs=-1),\n",
        "    \"bins\": 10,\n",
        "    \"kf\": 5,\n",
        "    \"clip_k\": 3.0,\n",
        "}\n"
      ],
      "metadata": {
        "id": "fs7O91DLcF-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the training and testing data.\n",
        "#The splitting was done randomly within specific quartiles by 80%-20% (discussed in the manuscript in detail).\n",
        "#The splitted data are also provided with the manuscript.\n",
        "\n",
        "train = pd.read_csv(\"Training_Data_Chlorophyll_BiasCorrection.csv\")\n",
        "test  = pd.read_csv(\"Test_Data_Chlorophyll_BiasCorrection.csv\")"
      ],
      "metadata": {
        "id": "lvCOhVnmcIuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the features and target variables from the loaded data\n",
        "\n",
        "X_train = train[CFG[\"features\"]].to_numpy()\n",
        "y_train = train[CFG[\"target\"]].to_numpy().astype(float)\n",
        "\n",
        "X_test  = test[CFG[\"features\"]].to_numpy()\n",
        "y_test  = test[CFG[\"target\"]].to_numpy().astype(float)"
      ],
      "metadata": {
        "id": "9ymRw0tjc7DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8kJiRIEaCxC"
      },
      "outputs": [],
      "source": [
        "#This cell presents the full demonstrattion of RF+VWLS. The code for a six steps procedure as detailed in the manuscript is shown here:\n",
        "\n",
        "\n",
        "#Step 1) Log transformation of target variable:\n",
        "\n",
        "y_train_log = np.log1p(np.maximum(y_train, 0.0))\n",
        "\n",
        "\n",
        "\n",
        "#Step 2) Training a pre-defined baseline RF model with a 5-fold cross validation on the training data (in log pace):\n",
        "\n",
        "#this model is only for estimating unbiased weights needed for the least square model trained in the subsequent steps)\n",
        "\n",
        "kf = KFold(n_splits=CFG[\"kf\"], shuffle=True, random_state=42)\n",
        "yhat_oof_log = np.zeros_like(y_train_log, dtype=float)\n",
        "\n",
        "for tr_idx, va_idx in kf.split(X_train):\n",
        "    m = RandomForestRegressor(**CFG[\"rf_base\"])\n",
        "    m.fit(X_train[tr_idx], y_train_log[tr_idx])\n",
        "    yhat_oof_log[va_idx] = m.predict(X_train[va_idx])\n",
        "\n",
        "eps_oof_log = y_train_log - yhat_oof_log #estimating out-of-fold (oof) residuals\n",
        "\n",
        "\n",
        "#Step 3)Estimating binned (or groupwise) weights for the weighted least square model (VWLS) as the inverse of the out-of-fold residuals:\n",
        "\n",
        "bins_log = np.quantile(y_train_log, np.linspace(0, 1, CFG[\"bins\"] + 1))\n",
        "bin_id = np.digitize(y_train_log, bins_log[1:-1], right=True)\n",
        "\n",
        "var_bins = np.array([\n",
        "    np.var(eps_oof_log[bin_id == i]) if np.any(bin_id == i) else np.nan\n",
        "    for i in range(CFG[\"bins\"])\n",
        "])\n",
        "var_bins = np.nan_to_num(var_bins, nan=np.nanmedian(var_bins))  #filling empty bins with median variance\n",
        "\n",
        "w = 1.0 / (var_bins[bin_id] + 1e-6) #estimating weights as the inverse of variance\n",
        "w /= np.mean(w)  #normalizing\n",
        "\n",
        "#Step 4) Developing and fitted the variance weighted least square (VWLS):\n",
        "\n",
        "#this VWLS model simulates residual by taking initial features and base model's predictions as input.\n",
        "#The VWLS is fitted on the out-of-fold residuals using the groupwise weights obtained in previous step.\n",
        "\n",
        "Xtr_ext_oof_log = np.column_stack([X_train, yhat_oof_log])\n",
        "Xtr_gls_oof_log = sm.add_constant(Xtr_ext_oof_log)\n",
        "\n",
        "vwls = sm.WLS(eps_oof_log, Xtr_gls_oof_log, weights=w).fit() #fitting the VWLS\n",
        "\n",
        "\n",
        "#Step 5) Re-training the final base RF model and correcting the predictions using VWLS:\n",
        "\n",
        "#this base RF model is trained on the full training data with same hyperparameters used before,\n",
        "#the trained RF here is applied to predict for test data (in log-space),\n",
        "#the fitted VWLS in previous step is the applied to simulate residuals for the test data as additive correction terms\n",
        "#the simulated residuls are then added back to the base RF predictions to obtain the corrected predictions.\n",
        "\n",
        "#retraining base RF\n",
        "rf_log = RandomForestRegressor(**CFG[\"rf_base\"])\n",
        "rf_log.fit(X_train, y_train_log)\n",
        "yhat_rf_log_te = rf_log.predict(X_test)\n",
        "\n",
        "\n",
        "#applying VWLS correction\n",
        "Xte_ext_log = np.column_stack([X_test, yhat_rf_log_te])\n",
        "Xte_gls_log = sm.add_constant(Xte_ext_log)\n",
        "eps_te_log = vwls.predict(Xte_gls_log)\n",
        "\n",
        "clip = CFG[\"clip_k\"] * np.std(eps_oof_log) #clipping for stability\n",
        "eps_te_log = np.clip(eps_te_log, -clip, clip)\n",
        "\n",
        "#correction in log-space\n",
        "yhat_log_te = yhat_rf_log_te + eps_te_log\n",
        "\n",
        "\n",
        "#Step 6) Back-transforming the corrected prediction in original scale:\n",
        "#back-transformation bias is reduced by multiplying a smearing factor which is computed from the out-fold residuals after correction\n",
        "# ----------------------------\n",
        "resid_oof_after = eps_oof_log - vwls.predict(Xtr_gls_oof_log)\n",
        "smear = float(np.mean(np.exp(resid_oof_after))) #smearing fatcor\n",
        "\n",
        "yhat_vwls = np.expm1(yhat_log_te) * smear\n",
        "\n",
        "print(\"VWLS-corrected predictions\", yhat_vwls)\n"
      ]
    }
  ]
}